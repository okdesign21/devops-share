## tree
.
├── all.txt
├── backend.hcl
├── cicd-lab-key.pem
├── ereaserdiagram.txt
├── .gitignore
├── keygen.tf
├── main.tf
├── modules
│   ├── alb
│   │   ├── main.tf
│   │   └── variables.tf
│   ├── ec2
│   │   ├── main.tf
│   │   └── variables.tf
│   ├── eks
│   │   ├── main.tf
│   │   ├── outputs.tf
│   │   └── variables.tf
│   ├── sg
│   │   ├── main.tf
│   │   └── variables.tf
│   ├── userdata
│   │   ├── common
│   │   │   ├── docker.sh
│   │   │   ├── nat.sh
│   │   │   ├── ssm.sh
│   │   │   └── swap.sh
│   │   ├── compose
│   │   │   ├── app.sh
│   │   │   ├── gitlab.sh
│   │   │   ├── jenkins_agent.sh
│   │   │   ├── jenkins_server.sh
│   │   │   └── prometheus.sh
│   │   ├── main.tf
│   │   ├── outputs.tf
│   │   └── variables.tf
│   └── vpc
│       ├── main.tf
│       ├── outputs.tf
│       └── variables.tf
├── outputs.tf
├── providers.tf
├── .terraform.lock.hcl
├── terraform.tfvars
└── variables.tf

10 directories, 36 files


==> /home/or/devops-share/backend.hcl <==
bucket         = "tfstate-inf-orinbar-euc1"
key            = "inf-devops/terraform.tfstate"
region         = "eu-central-1"
encrypt        = true
profile	       = "default"

==> /home/or/devops-share/keygen.tf <==
locals {
  use_generated_key  = var.key_name == null || var.key_name == ""
  effective_key_name = local.use_generated_key ? "${var.project_name}-key" : var.key_name
}

resource "tls_private_key" "gen" {
  count     = local.use_generated_key ? 1 : 0
  algorithm = "RSA"
  rsa_bits  = 4096
}

resource "aws_key_pair" "gen" {
  count      = local.use_generated_key ? 1 : 0
  key_name   = local.effective_key_name
  public_key = tls_private_key.gen[0].public_key_openssh
}

resource "local_file" "private_key_pem" {
  count           = local.use_generated_key ? 1 : 0
  content         = tls_private_key.gen[0].private_key_pem
  filename        = "${path.module}/${local.effective_key_name}.pem"
  file_permission = "0600"
}

output "ssh_private_key_path" {
  description = "Path to the generated private key (if created)"
  value       = local.use_generated_key ? local_file.private_key_pem[0].filename : null
  sensitive   = true
}

output "ssh_key_name" {
  value = local.effective_key_name
}

==> /home/or/devops-share/main.tf <==
# AMI: Ubuntu 24.04 LTS
data "aws_ssm_parameter" "ubuntu_24" {
  name = "/aws/service/canonical/ubuntu/server/24.04/stable/current/amd64/hvm/ebs-gp3/ami-id"
}

# SSM role & instance profile for Session Manager
resource "aws_iam_role" "ssm_ec2" {
  name = "${var.project_name}-ssm-ec2-role"
  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [{
      Effect    = "Allow",
      Principal = { Service = "ec2.amazonaws.com" },
      Action    = "sts:AssumeRole"
    }]
  })
}

resource "aws_iam_role_policy_attachment" "ssm_core" {
  role       = aws_iam_role.ssm_ec2.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore"
}

resource "aws_iam_instance_profile" "ssm" {
  name = "${var.project_name}-ssm-instance-profile"
  role = aws_iam_role.ssm_ec2.name
}

module "vpc" {
  source        = "./modules/vpc"
  name          = var.project_name
  cidr_block    = var.vpc_cidr
  azs           = ["${var.region}a", "${var.region}b"]
  public_cidrs  = var.public_cidrs
  private_cidrs = var.private_cidrs
}

module "sg" {
  source                   = "./modules/sg"
  vpc_id                   = module.vpc.vpc_id
  allowed_nat_ingress_cidr = var.vpc_cidr
}

# ---------- user-data bundles ----------
locals {
  alb_vars = <<-EOT
    #!/usr/bin/env bash
    export ALB_DNS="${module.alb.alb_dns_name}"
  EOT

  vpc_vars = <<-EOT
    #!/usr/bin/env bash
    export VPC_ID="${module.vpc.vpc_id}"
    export SUBNET_PUBLIC_A="${module.vpc.public_subnet_ids[0]}"
    export SUBNET_PUBLIC_B="${module.vpc.public_subnet_ids[1]}"
    export VPC_CIDR="${var.vpc_cidr}"
  EOT

  nat_vars = <<-EOT
    #!/usr/bin/env bash
    export PRIVATE_CIDRS="${var.vpc_cidr}" 
  EOT
}

module "ud_app" {
  source = "./modules/userdata"
  scripts = [
    "${path.module}/modules/userdata/common/swap.sh",
    "${path.module}/modules/userdata/common/docker.sh",
    "${path.module}/modules/userdata/common/ssm.sh",
    "${path.module}/modules/userdata/compose/app.sh"
  ]
}

module "ud_jenkins" {
  source = "./modules/userdata"
  scripts = [
    "${path.root}/modules/userdata/common/swap.sh",
    "${path.root}/modules/userdata/common/docker.sh",
    "${path.root}/modules/userdata/common/ssm.sh",
    "${path.root}/modules/userdata/compose/jenkins_server.sh",
  ]
  inline_snippets = [local.alb_vars]
}

module "ud_jenkins_agent" {
  source = "./modules/userdata"
  scripts = [
    "${path.root}/modules/userdata/common/swap.sh",
    "${path.root}/modules/userdata/common/docker.sh",
    "${path.root}/modules/userdata/common/ssm.sh",
    "${path.root}/modules/userdata/compose/jenkins_agent.sh",
  ]
  inline_snippets = [local.alb_vars]
}

module "ud_gitlab" {
  source = "./modules/userdata"
  scripts = [
    "${path.root}/modules/userdata/common/swap.sh",
    "${path.root}/modules/userdata/common/docker.sh",
    "${path.root}/modules/userdata/common/ssm.sh",
    "${path.root}/modules/userdata/compose/gitlab.sh",
  ]
  inline_snippets = [local.alb_vars, local.vpc_vars]
}

module "ud_prom" {
  source = "./modules/userdata"
  scripts = [
    "${path.module}/modules/userdata/common/swap.sh",
    "${path.module}/modules/userdata/common/docker.sh",
    "${path.module}/modules/userdata/common/ssm.sh",
    "${path.module}/modules/userdata/compose/prometheus.sh"
  ]
}

module "ud_nat" {
  source = "./modules/userdata"
  scripts = [
    "${path.root}/modules/userdata/common/ssm.sh",
    "${path.root}/modules/userdata/common/nat.sh",
  ]
  inline_snippets = [local.nat_vars]
}

# ---------- compute ----------
# NAT Instance for egress from private subnets
module "nat_instance" {
  source                   = "./modules/ec2"
  name                     = "${var.project_name}-nat"
  ami_id                   = data.aws_ssm_parameter.ubuntu_24.value
  subnet_id                = module.vpc.public_subnet_ids[0]
  sg_ids                   = [module.sg.sg_nat]
  key_name                 = local.effective_key_name
  instance_type            = var.nat_instance_type
  associate_public_ip      = true
  user_data                = module.ud_nat.content
  enable_source_dest_check = false
  root_volume_size_gb      = var.nat_disk_size_gb
  iam_instance_profile     = aws_iam_instance_profile.ssm.name

}

resource "aws_route" "private_nat" {
  for_each = {
    az1 = module.vpc.private_route_table_ids[0]
    az2 = module.vpc.private_route_table_ids[1]
  }

  route_table_id         = each.value
  destination_cidr_block = "0.0.0.0/0"
  network_interface_id   = module.nat_instance.primary_network_interface_id
}

# Public ALB with path routing to app/jenkins/prom/gitlab
module "alb" {
  source            = "./modules/alb"
  name              = var.project_name
  vpc_id            = module.vpc.vpc_id
  subnets           = module.vpc.public_subnet_ids
  security_group_id = module.sg.sg_alb
  routes = concat(
    var.app_count > 0 ? [
      { name = "app", path = "/", port = 8000, health_path = "/" }
    ] : [],
    [
      { name = "jenkins", path = "/jenkins*", port = 8080, health_path = "/jenkins/login" },
      { name = "gitlab", path = "/gitlab*", port = 8080, health_path = "/gitlab/users/sign_in" }
    ],
    var.enable_prometheus ? [
      { name = "prometheus", path = "/prom*", port = 9090, health_path = "/-/healthy" }
    ] : []
  )
}

# ----- app EC2s -----
module "app" {
  source   = "./modules/ec2"
  for_each = { for i in range(var.app_count) : i => i }

  name                 = "${var.project_name}-app-${each.key}"
  ami_id               = data.aws_ssm_parameter.ubuntu_24.value
  subnet_id            = element(module.vpc.private_subnet_ids, each.key % length(module.vpc.private_subnet_ids))
  sg_ids               = [module.sg.sg_app]
  key_name             = local.effective_key_name
  instance_type        = var.app_instance_type
  root_volume_size_gb  = var.app_disk_size_gb
  associate_public_ip  = false
  user_data            = module.ud_app.content
  iam_instance_profile = aws_iam_instance_profile.ssm.name
  depends_on           = [module.nat_instance, aws_route.private_nat]
}

resource "aws_lb_target_group_attachment" "app" {
  for_each         = module.app
  target_group_arn = module.alb.tg_arns["app"]
  target_id        = each.value.instance_id
  port             = 8000
}

# ----- jenkins -----
module "jenkins_server" {
  source               = "./modules/ec2"
  name                 = "${var.project_name}-jenkins-controller"
  ami_id               = data.aws_ssm_parameter.ubuntu_24.value
  subnet_id            = module.vpc.private_subnet_ids[0]
  sg_ids               = [module.sg.sg_jenkins_srv]
  key_name             = local.effective_key_name
  instance_type        = var.jenkins_controller_type
  associate_public_ip  = false
  user_data            = module.ud_jenkins.content
  root_volume_size_gb  = var.jenkins_controller_disk
  iam_instance_profile = aws_iam_instance_profile.ssm.name
  depends_on           = [module.nat_instance, aws_route.private_nat]
}

module "jenkins_agent" {
  source               = "./modules/ec2"
  name                 = "${var.project_name}-jenkins-agent"
  ami_id               = data.aws_ssm_parameter.ubuntu_24.value
  subnet_id            = module.vpc.private_subnet_ids[0]
  sg_ids               = [module.sg.sg_jenkins_agt]
  key_name             = local.effective_key_name
  instance_type        = var.jenkins_agent_type
  associate_public_ip  = false
  user_data            = module.ud_jenkins_agent.content
  root_volume_size_gb  = var.jenkins_agent_disk
  iam_instance_profile = aws_iam_instance_profile.ssm.name
  depends_on           = [module.nat_instance, aws_route.private_nat]
}

resource "aws_lb_target_group_attachment" "jenkins" {
  target_group_arn = module.alb.tg_arns["jenkins"]
  target_id        = module.jenkins_server.instance_id
  port             = 8080
}

# ----- gitlab -----
module "gitlab" {
  source               = "./modules/ec2"
  name                 = "${var.project_name}-gitlab"
  ami_id               = data.aws_ssm_parameter.ubuntu_24.value
  subnet_id            = module.vpc.private_subnet_ids[0]
  sg_ids               = [module.sg.sg_gitlab]
  key_name             = local.effective_key_name
  instance_type        = var.gitlab_type
  associate_public_ip  = false
  user_data            = module.ud_gitlab.content
  root_volume_size_gb  = var.gitlab_disk
  iam_instance_profile = aws_iam_instance_profile.ssm.name
  depends_on           = [module.nat_instance, aws_route.private_nat]
}

resource "aws_lb_target_group_attachment" "gitlab" {
  target_group_arn = module.alb.tg_arns["gitlab"]
  target_id        = module.gitlab.instance_id
  port             = 8080
}

# ----- prometheus (optional) -----
module "prometheus" {
  count                = var.enable_prometheus ? 1 : 0
  source               = "./modules/ec2"
  name                 = "${var.project_name}-prometheus"
  ami_id               = data.aws_ssm_parameter.ubuntu_24.value
  subnet_id            = module.vpc.private_subnet_ids[0]
  sg_ids               = [module.sg.sg_prom]
  key_name             = local.effective_key_name
  instance_type        = var.prometheus_type
  associate_public_ip  = false
  user_data            = module.ud_prom.content
  root_volume_size_gb  = var.prometheus_disk
  iam_instance_profile = aws_iam_instance_profile.ssm.name
  depends_on           = [module.nat_instance, aws_route.private_nat]
}

resource "aws_lb_target_group_attachment" "prometheus" {
  count            = var.enable_prometheus ? 1 : 0
  target_group_arn = module.alb.tg_arns["prometheus"]
  target_id        = module.prometheus[0].instance_id
  port             = 9090
}

module "eks" {
  source             = "./modules/eks"
  name               = var.project_name
  vpc_id             = module.vpc.vpc_id
  private_subnet_ids = module.vpc.private_subnet_ids
}

output "alb_dns" { value = module.alb.alb_dns_name }
output "jenkins_ip" { value = module.jenkins_server.private_ip }
output "gitlab_ip" { value = module.gitlab.private_ip }

==> /home/or/devops-share/modules/alb/main.tf <==
resource "aws_lb" "this" {
  name               = "${var.name}-alb"
  internal           = false
  load_balancer_type = "application"
  security_groups    = [var.security_group_id]
  subnets            = var.subnets
}

resource "aws_lb_listener" "http" {
  load_balancer_arn = aws_lb.this.arn
  port              = 80
  protocol          = "HTTP"
  default_action {
    type = "fixed-response"
    fixed_response {
      content_type = "text/plain"
      message_body = "Not Found"
      status_code  = "404"
    }
  }
}

resource "aws_lb_target_group" "tgs" {
  for_each = { for r in var.routes : r.name => r }
  name     = "${var.name}-${each.value.name}-tg"
  port     = each.value.port
  protocol = "HTTP"
  vpc_id   = var.vpc_id
  health_check {
    path    = each.value.health_path
    matcher = "200-399"
  }
}

resource "aws_lb_listener_rule" "rules" {
  for_each     = aws_lb_target_group.tgs
  listener_arn = aws_lb_listener.http.arn
  priority     = 10 + index(keys(aws_lb_target_group.tgs), each.key)
  action {
    type             = "forward"
    target_group_arn = each.value.arn
  }
  condition {
    path_pattern {
      values = [var.routes[index(keys(aws_lb_target_group.tgs), each.key)].path]
    }
  }
}

output "alb_dns_name" {
  value = aws_lb.this.dns_name
}

output "tg_arns" {
  value = { for k, v in aws_lb_target_group.tgs : k => v.arn }
}

==> /home/or/devops-share/modules/alb/variables.tf <==
variable "name" {
  type = string
}

variable "vpc_id" {
  type = string
}

variable "subnets" {
  type = list(string)
}

variable "security_group_id" {
  type = string
}

variable "routes" {
  description = "Path-based routes to backend ports"
  type = list(object({
    name        = string
    path        = string
    port        = number
    health_path = string
  }))
}

variable "enable_prometheus" {
  type    = bool
  default = false
}

==> /home/or/devops-share/modules/ec2/main.tf <==
resource "aws_instance" "this" {
  ami                         = var.ami_id
  instance_type               = var.instance_type
  key_name                    = var.key_name
  subnet_id                   = var.subnet_id
  vpc_security_group_ids      = var.sg_ids
  associate_public_ip_address = var.associate_public_ip
  user_data                   = var.user_data
  source_dest_check           = var.enable_source_dest_check
  iam_instance_profile        = var.iam_instance_profile

  root_block_device {
    volume_type = "gp3"
    volume_size = var.root_volume_size_gb
  }

  tags = { Name = var.name }
}

output "instance_id" {
  value = aws_instance.this.id
}

output "public_ip" {
  value = aws_instance.this.public_ip
}

output "private_ip" {
  value = aws_instance.this.private_ip
}

output "primary_network_interface_id" {
  value = aws_instance.this.primary_network_interface_id
}

==> /home/or/devops-share/modules/ec2/variables.tf <==
variable "name" {
  type = string
}

variable "ami_id" {
  type = string
}

variable "subnet_id" {
  type = string
}

variable "sg_ids" {
  type = list(string)
}

variable "key_name" {
  type    = string
  default = null
}

variable "instance_type" {
  type = string
}

variable "associate_public_ip" {
  type    = bool
  default = false
}

variable "user_data" {
  type    = string
  default = ""
}

variable "root_volume_size_gb" {
  type    = number
  default = 8
}

variable "enable_source_dest_check" {
  type    = bool
  default = true
}

variable "iam_instance_profile" {
  type    = string
  default = null
}

==> /home/or/devops-share/modules/eks/main.tf <==
locals {
  cluster_name = "${var.name}-eks"
}

module "eks_core" {
  source             = "terraform-aws-modules/eks/aws"
  version            = "21.3.1"
  name               = local.cluster_name
  kubernetes_version = var.cluster_version

  vpc_id                   = var.vpc_id
  subnet_ids               = var.private_subnet_ids
  control_plane_subnet_ids = var.private_subnet_ids

  endpoint_private_access = true
  endpoint_public_access  = false

  enable_irsa                              = true
  enable_cluster_creator_admin_permissions = true

  create_security_group      = true
  create_node_security_group = true
  #security_group_id          = null
  #node_security_group_id     = null
  create_kms_key             = false
  #cluster_encryption_config = null

  addons = {
    coredns    = {}
    kube-proxy = {}
    vpc-cni    = { before_compute = true }
  }

  eks_managed_node_groups = {
    default = {
      name           = "${local.cluster_name}-mng"
      instance_types = [var.node_instance_type]
      desired_size   = var.node_desired_size
      min_size       = var.node_min_size
      max_size       = var.node_max_size
      capacity_type  = var.node_capacity_type
      subnet_ids     = var.private_subnet_ids

      create_iam_role = var.node_iam_role_arn == null ? true : false
      iam_role_arn    = var.node_iam_role_arn

      iam_role_additional_policies = var.node_iam_additional_policies
      labels                       = var.node_labels
    }
  }

  tags = merge({ Project = var.name, Stack = "eks" }, var.tags)
}

==> /home/or/devops-share/modules/eks/outputs.tf <==
output "cluster_name" {
  value = module.eks_core.cluster_name
}

output "cluster_endpoint" {
  value = module.eks_core.cluster_endpoint
}

output "cluster_security_group_id" {
  value = module.eks_core.cluster_security_group_id
}

output "oidc_provider_arn" {
  value = module.eks_core.oidc_provider_arn
}

output "node_group_role_name" {
  value = module.eks_core.eks_managed_node_groups["default"].iam_role_name
}

==> /home/or/devops-share/modules/eks/variables.tf <==
variable "name" {
  description = "Base name/prefix (e.g., project_name)"
  type        = string
}

variable "vpc_id" {
  description = "VPC ID where the cluster will live"
  type        = string
}

variable "private_subnet_ids" {
  description = "Private subnet IDs for control plane and nodes"
  type        = list(string)
}

variable "cluster_version" {
  description = "EKS version (e.g., 1.33)."
  type        = string
  default     = "1.33"
}

# Node group variables
variable "node_instance_type" {
  description = "EC2 instance type for worker nodes"
  type        = string
  default     = "t3.small"
}

variable "node_desired_size" {
  description = "Desired number of worker nodes"
  type        = number
  default     = 2
}

variable "node_min_size" {
  description = "Minimum number of worker nodes"
  type        = number
  default     = 2
}

variable "node_max_size" {
  description = "Maximum number of worker nodes"
  type        = number
  default     = 4
}

variable "node_capacity_type" {
  description = "Capacity type for worker nodes (ON_DEMAND or SPOT)"
  type        = string
  default     = "ON_DEMAND"
}

variable "tags" {
  description = "Common tags to apply"
  type        = map(string)
  default     = {}
}

# Node labels / taints as variables
variable "node_labels" {
  description = "Labels applied to the managed node group"
  type        = map(string)
  default     = { role = "general" }
}

# Additional IAM policies to attach to the node role
variable "node_iam_additional_policies" {
  description = "Map of name=>policy ARN to attach to node IAM role"
  type        = map(string)
  default = {
    ssm = "arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore"
  }
}

# Use an existing IAM role for the node group (prevents recreation)
# If provided, Terraform will NOT create a role and will use this ARN.
variable "node_iam_role_arn" {
  description = "Existing IAM role ARN for the managed node group (optional)"
  type        = string
  default     = null
}

==> /home/or/devops-share/modules/sg/main.tf <==
resource "aws_security_group" "alb" {
  name   = "alb-sg"
  vpc_id = var.vpc_id
  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

locals {
  alb_only = [aws_security_group.alb.id]
}

resource "aws_security_group" "app" {
  name   = "app-sg"
  vpc_id = var.vpc_id
  ingress {
    from_port       = 8000
    to_port         = 8000
    protocol        = "tcp"
    security_groups = local.alb_only
  }
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

resource "aws_security_group" "jenkins_srv" {
  name   = "jenkins-srv-sg"
  vpc_id = var.vpc_id
  ingress {
    from_port       = 8080
    to_port         = 8080
    protocol        = "tcp"
    security_groups = local.alb_only
  }
  ingress {
    from_port = 50000
    to_port   = 50000
    protocol  = "tcp"
  }
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

resource "aws_security_group" "jenkins_agt" {
  name   = "jenkins-agt-sg"
  vpc_id = var.vpc_id
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

resource "aws_security_group_rule" "jnlp_from_agent" {
  type                     = "ingress"
  from_port                = 50000
  to_port                  = 50000
  protocol                 = "tcp"
  source_security_group_id = aws_security_group.jenkins_agt.id
  security_group_id        = aws_security_group.jenkins_srv.id
}

resource "aws_security_group" "gitlab" {
  name   = "gitlab-sg"
  vpc_id = var.vpc_id
  ingress {
    from_port       = 8080
    to_port         = 8080
    protocol        = "tcp"
    security_groups = local.alb_only
  }
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

resource "aws_security_group" "prom" {
  name   = "prom-sg"
  vpc_id = var.vpc_id
  ingress {
    from_port       = 9090
    to_port         = 9090
    protocol        = "tcp"
    security_groups = local.alb_only
  }
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

resource "aws_security_group" "nat" {
  name   = "nat-sg"
  vpc_id = var.vpc_id
  ingress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = [var.allowed_nat_ingress_cidr]
  }
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

output "sg_alb" {
  value = aws_security_group.alb.id
}

output "sg_app" {
  value = aws_security_group.app.id
}

output "sg_jenkins_srv" {
  value = aws_security_group.jenkins_srv.id
}

output "sg_jenkins_agt" {
  value = aws_security_group.jenkins_agt.id
}

output "sg_gitlab" {
  value = aws_security_group.gitlab.id
}

output "sg_prom" {
  value = aws_security_group.prom.id
}

output "sg_nat" {
  value = aws_security_group.nat.id
}

==> /home/or/devops-share/modules/sg/variables.tf <==
variable "vpc_id" {
  type = string
}

variable "allowed_nat_ingress_cidr" {
  description = "CIDR allowed to reach the NAT (typically the VPC CIDR)"
  type        = string
}


==> /home/or/devops-share/modules/userdata/common/docker.sh <==
#!/usr/bin/env bash
set -euxo pipefail

export DEBIAN_FRONTEND=noninteractive

retry() {
  local n=0 max=${2:-8} delay=${3:-5}
  until bash -lc "$1"; do
    n=$((n+1))
    if [ "$n" -ge "$max" ]; then
      echo "Command failed after $n attempts: $1"
      exit 1
    fi
    sleep "$delay"
  done
}

# Update and install Docker + curl (with retries to survive first-boot races)
retry "apt-get update"
retry "apt-get install -y docker.io curl"

# Start Docker
systemctl enable --now docker || systemctl enable --now docker.service

if ! dpkg -s docker-compose-plugin >/dev/null 2>&1; then
  retry "apt-get install -y docker-compose-plugin"
fi

for u in ubuntu ssm-user ec2-user; do
  if id "$u" >/dev/null 2>&1; then
    usermod -aG docker "$u" || true
  fi
done
systemctl restart docker || true

==> /home/or/devops-share/modules/userdata/common/nat.sh <==
#!/usr/bin/env bash

# Optional: PRIVATE_CIDRS exported by TF; default to RFC1918
PRIVATE_CIDRS="${PRIVATE_CIDRS:-10.0.0.0/8 172.16.0.0/12 192.168.0.0/16}"

# Ensure packages are present (iptables + persistence helpers)
export DEBIAN_FRONTEND=noninteractive
apt-get update -y
apt-get install -y iptables iptables-persistent netfilter-persistent

# Enable IP forwarding (persistent)
sed -i '/^net.ipv4.ip_forward/d' /etc/sysctl.conf
echo "net.ipv4.ip_forward=1" >/etc/sysctl.conf
sysctl -p

# Write the NAT config script (logs to /var/log/nat-bootstrap.log)
install -d -m 0755 /usr/local/bin
cat >/usr/local/bin/configure-nat.sh <<'EOF'
#!/usr/bin/env bash
set -euxo pipefail
exec > >(tee -a /var/log/nat-bootstrap.log) 2>&1

PRIVATE_CIDRS="${PRIVATE_CIDRS:-10.0.0.0/8 172.16.0.0/12 192.168.0.0/16}"
NIC="$(ip -o -4 route show to default | awk '{print $5}' || true)"
NIC="${NIC:-eth0}"

# Add MASQUERADE rules (idempotent)
for cidr in ${PRIVATE_CIDRS}; do
  iptables -t nat -C POSTROUTING -s "${cidr}" -o "${NIC}" -j MASQUERADE 2>/dev/null \
    || iptables -t nat -A POSTROUTING -s "${cidr}" -o "${NIC}" -j MASQUERADE
done

# Save rules & ensure persistence
iptables-save >/etc/iptables/rules.v4
systemctl enable netfilter-persistent
systemctl restart netfilter-persistent || true
EOF
chmod +x /usr/local/bin/configure-nat.sh

# Create a systemd unit that runs after network is up
cat >/etc/systemd/system/nat-snat.service <<'EOF'
[Unit]
Description=Configure NAT SNAT (MASQUERADE)
Wants=network-online.target
After=network-online.target

[Service]
Type=oneshot
EnvironmentFile=-/etc/default/nat-snat
ExecStart=/usr/local/bin/configure-nat.sh
RemainAfterExit=yes

[Install]
WantedBy=multi-user.target
EOF

# Export PRIVATE_CIDRS to the unit (optional)
mkdir -p /etc/default
echo "PRIVATE_CIDRS=\"${PRIVATE_CIDRS}\"" >/etc/default/nat-snat

# Enable & start
systemctl daemon-reload
systemctl enable nat-snat.service
systemctl start nat-snat.service


==> /home/or/devops-share/modules/userdata/common/ssm.sh <==
#!/bin/bash

# Prefer snap on Ubuntu 20.04+; falls back to systemd unit enable
if ! command -v amazon-ssm-agent >/dev/null 2>&1; then
  if command -v snap >/dev/null 2>&1; then
    snap install amazon-ssm-agent --classic
  else
    # minimal fallback using apt repo
    apt-get update
    apt-get install -y amazon-ssm-agent
  fi
fi
systemctl enable --now snap.amazon-ssm-agent.amazon-ssm-agent || systemctl enable --now amazon-ssm-agent || true

==> /home/or/devops-share/modules/userdata/common/swap.sh <==
#!/bin/bash

if ! grep -q '/swapfile' /etc/fstab; then
  fallocate -l 2G /swapfile || true
  chmod 600 /swapfile || true
  mkswap /swapfile || true
  swapon /swapfile || true
  echo '/swapfile none swap sw 0 0' >> /etc/fstab
fi

==> /home/or/devops-share/modules/userdata/compose/app.sh <==
#!/bin/bash
set -euxo pipefail
mkdir -p /opt/app
cat > /opt/app/docker-compose.yml <<'YML'
version: "3.8"
services:
  app:
    image: okdesign21/weather_app:current
    ports:
      - "8000:8000"
    restart: unless-stopped
YML

if command -v docker-compose >/dev/null 2>&1; then
  docker-compose -f /opt/app/docker-compose.yml up -d
else
  docker compose -f /opt/app/docker-compose.yml up -d
fi

==> /home/or/devops-share/modules/userdata/compose/gitlab.sh <==
#!/bin/bash
set -euxo pipefail

mkdir -p /opt/gitlab
cat > /opt/gitlab/docker-compose.yml <<'YML'
version: "3.8"
services:
  gitlab:
    image: gitlab/gitlab-ce:latest
    container_name: gitlab
    restart: always
    hostname: gitlab.local
    shm_size: "256m"
    ports:
      - "8080:80"
      - "443:443"
      - "2222:22"
    volumes:
      - ./config:/etc/gitlab
      - ./logs:/var/log/gitlab
      - ./data:/var/opt/gitlab
      - ./git_home:/home/git
    environment:
      GITLAB_OMNIBUS_CONFIG: |
        external_url 'http://localhost:8080'
        nginx['listen_https'] = false
        nginx['listen_port'] = 80
        nginx['redirect_http_to_https'] = false
        gitlab_rails['gitlab_relative_url_root'] = '/gitlab'

        gitlab_rails['allow_local_requests_from_web_hooks_and_services'] = true
        gitlab_rails['allow_local_requests_from_system_hooks'] = true
        gitlab_rails['outbound_local_requests_whitelist'] = ['${VPC_CIDR}']

        gitlab_rails['gitlab_shell_ssh_port'] = 2222
        gitlab_rails['trusted_proxies'] = ['${VPC_CIDR}']
        gitlab_rails['trusted_ip_whitelist'] = ['${VPC_CIDR}']

    logging:
      driver: "local"
      options:
        max-size: "10m"
        max-file: "3"
volumes:
  config:
  logs:
  data:
  git_home:
YML

cd /opt/gitlab && /usr/local/bin/docker compose up -d


==> /home/or/devops-share/modules/userdata/compose/jenkins_agent.sh <==
#!/bin/bash
set -euxo pipefail

mkdir -p /opt/jenkins-agent
cat > /opt/jenkins-agent/Dockerfile <<'DOCKER'
FROM jenkins/inbound-agent:latest
USER root
RUN curl -fsSL https://get.docker.com | sh \
    && groupadd -g 988 docker || true \
    && usermod -aG docker jenkins
RUN apt-get update && apt-get install -y git curl && apt-get clean && rm -rf /var/lib/apt/lists/*
USER jenkins
DOCKER

cat > /opt/jenkins-agent/docker-compose.yml <<'YML'
version: "3.8"
services:
  jenkins-agent:
    container_name: jenkins-agent-docker
    build: .
    restart: always
    environment:
      - JENKINS_URL=http://${ALB_DNS}/jenkins
      - JENKINS_AGENT_NAME=docker
      - JENKINS_SECRET=__REPLACE_ME__
      - JENKINS_WEB_SOCKET=true
      - JENKINS_AGENT_WORKDIR=/home/jenkins/agent
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - agent_data:/home/jenkins/agent
    working_dir: /home/jenkins/agent
    logging:
      driver: local
      options:
        max-size: "10m"
        max-file: "3"
volumes:
  agent_data:
YML

cd /opt/jenkins-agent && /usr/local/bin/docker-compose up -d


==> /home/or/devops-share/modules/userdata/compose/jenkins_server.sh <==
#!/bin/bash
set -euxo pipefail

mkdir -p /opt/jenkins
cat > /opt/jenkins/docker-compose.yml <<'YML'
version: "3.8"
services:
  jenkins:
    image: jenkins/jenkins:lts
    container_name: jenkins
    user: root
    ports:
      - "8080:8080"
      - "50000:50000"
    environment:
      - JAVA_OPTS=-Djenkins.model.Jenkins.crumbIssuerProxyCompatibility=true
      - GITLAB_URL=http://${ALB_DNS}/gitlab	
      - JENKINS_OPTS=--prefix=/jenkins
    volumes:
      - jenkins_home:/var/jenkins_home
    restart: always
volumes:
  jenkins_home:
YML
cd /opt/jenkins && /usr/local/bin/docker-compose up -d


==> /home/or/devops-share/modules/userdata/compose/prometheus.sh <==
#!/bin/bash

mkdir -p /opt/prometheus
cat > /opt/prometheus/docker-compose.yml <<'YML'
version: "3.8"
services:
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
    restart: unless-stopped
YML
cat > /opt/prometheus/prometheus.yml <<'CFG'
global:
  scrape_interval: 15s
scrape_configs:
  - job_name: 'self'
    static_configs:
      - targets: ['localhost:9090']
CFG
cd /opt/prometheus && /usr/local/bin/docker-compose up -d

==> /home/or/devops-share/modules/userdata/main.tf <==
locals {
  static_contents = [for p in var.scripts : file(abspath(p))]
  contents        = concat(var.inline_snippets, local.static_contents)
}


==> /home/or/devops-share/modules/userdata/outputs.tf <==
output "content" {
  description = "Concatenated user_data content"
  value       = join(var.separator, local.contents)
}

==> /home/or/devops-share/modules/userdata/variables.tf <==
variable "scripts" {
  description = "List of local script paths to concatenate into user_data"
  type        = list(string)
}

variable "inline_snippets" {
  type        = list(string)
  default     = []
  description = "Raw shell script snippets to prepend/append inline."
}

variable "separator" {
  type    = string
  default = "\n\n"
}

==> /home/or/devops-share/modules/vpc/main.tf <==
resource "aws_vpc" "this" {
  cidr_block           = var.cidr_block
  enable_dns_hostnames = true
  enable_dns_support   = true
  tags                 = { Name = var.name }
}

resource "aws_internet_gateway" "igw" {
  vpc_id = aws_vpc.this.id
  tags   = { Name = "${var.name}-igw" }
}

resource "aws_subnet" "public" {
  for_each                = { for idx, cidr in var.public_cidrs : idx => { cidr = cidr, az = var.azs[idx] } }
  vpc_id                  = aws_vpc.this.id
  cidr_block              = each.value.cidr
  availability_zone       = each.value.az
  map_public_ip_on_launch = true
  tags                    = { Name = "${var.name}-public-${each.key}" }
}

resource "aws_subnet" "private" {
  for_each          = { for idx, cidr in var.private_cidrs : idx => { cidr = cidr, az = var.azs[idx] } }
  vpc_id            = aws_vpc.this.id
  cidr_block        = each.value.cidr
  availability_zone = each.value.az
  tags              = { Name = "${var.name}-private-${each.key}" }
}

resource "aws_route_table" "public" {
  vpc_id = aws_vpc.this.id
  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.igw.id
  }
  tags = { Name = "${var.name}-public-rt" }
}

resource "aws_route_table_association" "public" {
  for_each       = aws_subnet.public
  subnet_id      = each.value.id
  route_table_id = aws_route_table.public.id
}

resource "aws_route_table" "private" {
  count  = length(var.private_cidrs)
  vpc_id = aws_vpc.this.id
  tags   = { Name = "${var.name}-private-rt-${count.index}" }
}

resource "aws_route_table_association" "private" {
  for_each       = aws_subnet.private
  subnet_id      = each.value.id
  route_table_id = element(aws_route_table.private[*].id, tonumber(each.key))
}

==> /home/or/devops-share/modules/vpc/outputs.tf <==
output "vpc_id" {
  value = aws_vpc.this.id
}

output "public_subnet_ids" {
  value = [for s in aws_subnet.public : s.id]
}

output "private_subnet_ids" {
  value = [for s in aws_subnet.private : s.id]
}

output "private_route_table_ids" {
  value = aws_route_table.private[*].id
}

==> /home/or/devops-share/modules/vpc/variables.tf <==
variable "name" {
  type = string
}

variable "cidr_block" {
  type = string
}

variable "azs" {
  type = list(string)
}

variable "public_cidrs" {
  type = list(string)
}

variable "private_cidrs" {
  type = list(string)
}

==> /home/or/devops-share/outputs.tf <==
output "alb_dns_name" {
  value = module.alb.alb_dns_name
}

output "app_url" {
  value = "http://${module.alb.alb_dns_name}/"
}

output "jenkins_url" {
  value = "http://${module.alb.alb_dns_name}/jenkins"
}

output "gitlab_url" {
  value = "http://${module.alb.alb_dns_name}/gitlab"
}

output "prometheus_url" {
  value = var.enable_prometheus ? "http://${module.alb.alb_dns_name}/prom" : null
}

output "gitlab_private_ip" {
  value = module.gitlab.private_ip
}

output "jenkins_controller_ip" {
  value = module.jenkins_server.private_ip
}

==> /home/or/devops-share/providers.tf <==
terraform {
  required_version = ">= 1.6.0"

  // backend "s3" {}

  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = ">= 6.13.0"
    }
    tls = {
      source  = "hashicorp/tls"
      version = ">= 4.0.0"
    }
    local = {
      source  = "hashicorp/local"
      version = ">= 2.4.0"
    }
  }
}

provider "aws" {
  region = var.region
}

==> /home/or/devops-share/terraform.tfvars <==
# General
project_name = "cicd-lab"
region       = "eu-central-1"

# VPC & subnets (parametrized)
vpc_cidr      = "10.0.0.0/16"
public_cidrs  = ["10.0.1.0/24", "10.0.3.0/24"]
private_cidrs = ["10.0.2.0/24", "10.0.4.0/24"]

# Key handling
# Leave empty to auto-generate an SSH keypair
key_name = ""

enable_prometheus = false

nat_instance_type       = "m7i-flex.large"
gitlab_type             = "m7i-flex.large"
jenkins_controller_type = "m7i-flex.large"
jenkins_agent_type      = "m7i-flex.large"
app_instance_type       = "m7i-flex.large"
# prometheus_instance_type = "m7i-flex.large"

==> /home/or/devops-share/variables.tf <==
variable "region" {
  type    = string
  default = "eu-central-1"
}

variable "project_name" {
  type    = string
  default = "smart-pipeline"
}

variable "nat_instance_type" {
  type    = string
  default = "t3.nano"
}

variable "nat_disk_size_gb" {
  type    = number
  default = 8
}

variable "app_count" {
  type    = number
  default = 2
}

variable "app_instance_type" {
  type    = string
  default = "t3.micro"
}

variable "app_disk_size_gb" {
  type    = number
  default = 8
}

variable "jenkins_controller_type" {
  type    = string
  default = "t3.medium"
}

variable "jenkins_controller_disk" {
  type    = number
  default = 8
}

variable "jenkins_agent_type" {
  type    = string
  default = "t3.small"
}

variable "jenkins_agent_disk" {
  type    = number
  default = 8
}

variable "gitlab_type" {
  type    = string
  default = "t3.large"
}

variable "gitlab_disk" {
  type    = number
  default = 20
}

variable "prometheus_type" {
  type    = string
  default = "t3.small"
}

variable "prometheus_disk" {
  type    = number
  default = 8
}

variable "enable_prometheus" {
  type    = bool
  default = false
}

variable "key_name" {
  description = "If null/empty, Terraform generates a new key pair"
  type        = string
  default     = null
}

variable "vpc_cidr" {
  description = "VPC CIDR"
  type        = string
  default     = "10.0.0.0/16"
}

variable "public_cidrs" {
  description = "Public subnet CIDRs, one per AZ"
  type        = list(string)
  default     = ["10.0.1.0/24", "10.0.3.0/24"]
}

variable "private_cidrs" {
  description = "Private subnet CIDRs, one per AZ"
  type        = list(string)
  default     = ["10.0.2.0/24", "10.0.4.0/24"]
}
